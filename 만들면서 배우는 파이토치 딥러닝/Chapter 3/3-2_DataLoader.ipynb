{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOMAxiB9XXq1ZoyWSlPvxu5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!git clone https://github.com/Rope-player/pytorch_advanced.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uqrl2a7vvVYT","executionInfo":{"status":"ok","timestamp":1664161391609,"user_tz":-540,"elapsed":4040,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"9f3c2a89-4a19-4cb6-d575-6fc4216d66ba"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'pytorch_advanced'...\n","remote: Enumerating objects: 548, done.\u001b[K\n","remote: Counting objects: 100% (174/174), done.\u001b[K\n","remote: Compressing objects: 100% (173/173), done.\u001b[K\n","remote: Total 548 (delta 5), reused 162 (delta 0), pack-reused 374\u001b[K\n","Receiving objects: 100% (548/548), 50.13 MiB | 29.28 MiB/s, done.\n","Resolving deltas: 100% (43/43), done.\n"]}]},{"cell_type":"code","source":["%cd \"pytorch_advanced\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBn4TxYgvcHV","executionInfo":{"status":"ok","timestamp":1664161391609,"user_tz":-540,"elapsed":5,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"e78b5152-45b0-42e8-d650-3aa0031341b7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pytorch_advanced\n"]}]},{"cell_type":"code","source":["%cd \"3_semantic_segmentation\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-14sh22vcUU","executionInfo":{"status":"ok","timestamp":1664161391610,"user_tz":-540,"elapsed":4,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"274aa7c6-dd84-4533-b3e1-97f93e7a1b6d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pytorch_advanced/3_semantic_segmentation\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"c5271VBfut0g","executionInfo":{"status":"ok","timestamp":1664161391610,"user_tz":-540,"elapsed":3,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"outputs":[],"source":["import os\n","import urllib.request\n","import zipfile\n","import tarfile"]},{"cell_type":"code","source":["data_dir = \"./data/\"\n","if not os.path.exists(data_dir):\n","  os.mkdir(data_dir)"],"metadata":{"id":"z40jXH1hu77J","executionInfo":{"status":"ok","timestamp":1664161392114,"user_tz":-540,"elapsed":3,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["weights_dir = \"./weights/\"\n","if not os.path.exists(weights_dir):\n","  os.mkdir(weights_dir)"],"metadata":{"id":"Fz642zNuu7-Y","executionInfo":{"status":"ok","timestamp":1664161392115,"user_tz":-540,"elapsed":3,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n","target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n","\n","if not os.path.exists(target_path):\n","  urllib.request.urlretrieve(url, target_path)\n","    \n","  tar = tarfile.TarFile(target_path)  \n","  tar.extractall(data_dir)  \n","  tar.close()"],"metadata":{"id":"pjzaM5L_u8Co","executionInfo":{"status":"ok","timestamp":1664161469451,"user_tz":-540,"elapsed":77338,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import os.path as osp\n","from PIL import Image\n","\n","import torch.utils.data as data"],"metadata":{"id":"n7BzROl1wS2y","executionInfo":{"status":"ok","timestamp":1664161767447,"user_tz":-540,"elapsed":3372,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 시맨틱 분할"],"metadata":{"id":"JOZHXelpxayb"}},{"cell_type":"markdown","source":["## 화상 데이터 및 어노테이션 파일의 경로 리스트 작성"],"metadata":{"id":"6HLMNglqvBQY"}},{"cell_type":"code","source":["def make_datapath_list(rootpath):\n","  # 학습 및 검증을 위한 화상 데이터와 어노테이션 데이터 파일 경로 리스트 작성\n","  # Parameters\n","  #   rootpath : str - 데이터 폴더 경로\n","  # Returns\n","  #   ret : train_img_list, train_anno_list, val_img_list, val_anno_list - 데이터 경로를 지정한 리스트\n","\n","  # 화상 파일과 어노테이션 파일의 경로 템플릿 작성\n","  imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n","  annopath_template = osp.join(rootpath, 'SegmentationClass', '%s.png')\n","\n","  # 훈련 및 검증 파일 각각으 ID 취득\n","  train_id_names = osp.join(rootpath + 'ImageSets/Segmentation/train.txt')\n","  val_id_names = osp.join(rootpath + 'ImageSets/Segmentation/val.txt')\n","\n","  # 훈련 데이터의 화상과 어노테이션의 경로 리스트 작성\n","  train_img_list = list()\n","  train_anno_list = list()\n","\n","  for line in open(train_id_names):\n","    file_id = line.strip()                     # 공백과 줄바꿈 제거\n","    img_path = (imgpath_template % file_id)    # 화상 경로\n","    anno_path = (annopath_template % file_id)  # 어노테이션 경로\n","    train_img_list.append(img_path)\n","    train_anno_list.append(anno_path)\n","\n","  # 검증 데이터의 화상 파일과 어노테이션의 경로 리스트 작성\n","  val_img_list = list()\n","  val_anno_list = list()\n","\n","  for line in open(val_id_names):\n","    file_id = line.strip()  \n","    img_path = (imgpath_template % file_id)  \n","    anno_path = (annopath_template % file_id)\n","    val_img_list.append(img_path)\n","    val_anno_list.append(anno_path)\n","\n","  return train_img_list, train_anno_list, val_img_list, val_anno_list"],"metadata":{"id":"Pwq9WbmhvAg4","executionInfo":{"status":"ok","timestamp":1664161770051,"user_tz":-540,"elapsed":293,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 동작 확인\n","rootpath = \"./data/VOCdevkit/VOC2012/\"\n","\n","train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n","\n","print(train_img_list[0])\n","print(train_anno_list[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbRCuW5yxPzj","executionInfo":{"status":"ok","timestamp":1664161772477,"user_tz":-540,"elapsed":309,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"1b1331ac-5f52-42f0-fe79-a26e8999b4cb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["./data/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg\n","./data/VOCdevkit/VOC2012/SegmentationClass/2007_000032.png\n"]}]},{"cell_type":"markdown","source":["## 데이터셋 작성"],"metadata":{"id":"_nyh5AqdxYIS"}},{"cell_type":"markdown","source":["화상과 어노태이션을 전처리하는 `DataTransform` 클래스 작성후 `Dataset` 클래스 작성. 먼저 화상 데이터와 어노테이션을 데이터 세트로 변환.\n","\n","이어서 훈련데이터의 데이터 확장. 화상 크기를 `Scale` 클래스로 변형. 크기가 커졌다면 잘라내고 작아졌다면 여백을 검은색으로 채움.\n","\n","그리고 `RandomRoataion` 클래스로 회전을 시키고 `RandomMirror` 클래스로 좌우를 반전 시킴.\n","\n","`Resize` 클래스로 지정된 화상크기로 변환하고, `Normalize_Tensor` 클래스로 텐서형식으로 변환 시킴.\n","\n"],"metadata":{"id":"dVfBMV8ixgcF"}},{"cell_type":"markdown","source":["검증 데이터는 데이터 확장을 실시하지 않고 `Resize` 클래스부터 시작함"],"metadata":{"id":"JqbpK__8yc07"}},{"cell_type":"code","source":["from utils.data_augumentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor\n","\n","class DataTransform():\n","  # 화상과 어노테이션의 전처리 클래스, 훈려시와 검증시 다르게 동작.\n","  # 화상 크기를 input_sizexinput_size로 함. 훈련 시 데이터 학장 수행\n","  #\n","  # Attributes\n","  #   input_size : int       - 리사이즈 대상 크기\n","  #   color_mean : (R, G, B) - 각 색상 채널의 평균 값\n","  #   color_std  : (R, G, B) - 각 색상 채널의 표준 편차\n","\n","  def __init__(self, input_size, color_mean, color_std):\n","    self.data_transform = {\n","      'train': Compose([\n","        Scale(scale=[0.5, 1.5]),                 # 확대, 축소\n","        RandomRotation(angle=[-10, 10]),         # 회전\n","        RandomMirror(),                          # 랜덤 반전\n","        Resize(input_size),                      # 리사이즈(input_size)\n","        Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서화\n","      ]),\n","      'val': Compose([\n","        Resize(input_size),                      # 리사이즈(input_size)\n","        Normalize_Tensor(color_mean, color_std)  # 색상 정보의 표준화와 텐서화\n","      ])\n","    }\n","  \n","  def __call__(self, phase, img, anno_class_img):\n","    # Parameters\n","    #   phase : 'train' or 'val' - 전처리 모드 지정.\n","\n","    return self.data_transform[phase](img, anno_class_img)"],"metadata":{"id":"rZIsT9eyxflN","executionInfo":{"status":"ok","timestamp":1664162418316,"user_tz":-540,"elapsed":698,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Dataset 클래스인 VOCDataset 클래스 작성"],"metadata":{"id":"_byocXMcz4C8"}},{"cell_type":"code","source":["class VOCDataset(data.Dataset):\n","    # VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n","    #\n","    # Attributes\n","    #   img_list  : 리스트 - 어노테이션 경로 리스트\n","    #   anno_list : 리스트 - 어노테이션 경로 리스트\n","    #   phase     : 'train' or 'test' - 학습, 훈련 설정\n","    #   transform : object - 전처리 클래스 인스턴스\n","\n","    def __init__(self, img_list, anno_list, phase, transform):\n","      self.img_list = img_list\n","      self.anno_list = anno_list\n","      self.phase = phase\n","      self.transform = transform\n","\n","    def __len__(self):\n","      # 화상 매수 반환\n","      return len(self.img_list)\n","\n","    def __getitem__(self, index):\n","      # 전처리한 화상의 텐서 형식 데이터와 어노테이션 취득\n","      img, anno_class_img = self.pull_item(index)\n","      return img, anno_class_img\n","\n","    def pull_item(self, index):\n","      # 화상의 텐서 형식 데이터와 어노테이션 취득\n","\n","      # 1. 화상 읽기\n","      image_file_path = self.img_list[index]\n","      img = Image.open(image_file_path)   # [높이][폭][RGB]\n","\n","      # 2. 어노테이션 화상 읽기\n","      anno_file_path = self.anno_list[index]\n","      anno_class_img = Image.open(anno_file_path)   # [높이][폭]\n","\n","      # 3. 전처리 실시\n","      img, anno_class_img = self.transform(self.phase, img, anno_class_img)\n","\n","      return img, anno_class_img"],"metadata":{"id":"z7QZtrisz1Wl","executionInfo":{"status":"ok","timestamp":1664162682027,"user_tz":-540,"elapsed":289,"user":{"displayName":"J지원","userId":"16050027595319260781"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 동작 확인\n","\n","# (RGB)의 평균치와 표준편차\n","color_mean = (0.485, 0.456, 0.406)\n","color_std = (0.229, 0.224, 0.225)\n","\n","# 데이터셋 작성\n","train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n","val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(input_size=475, color_mean=color_mean, color_std=color_std))\n","\n","# 데이터 추출 예\n","print(val_dataset.__getitem__(0)[0].shape)\n","print(val_dataset.__getitem__(0)[1].shape)\n","print(val_dataset.__getitem__(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xVj7K4J02CZ","executionInfo":{"status":"ok","timestamp":1664162734366,"user_tz":-540,"elapsed":772,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"f8bb07de-3665-4a9b-f7d4-55a684e21ac6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 475, 475])\n","torch.Size([475, 475])\n","(tensor([[[ 1.6667,  1.5125,  1.5639,  ...,  1.7523,  1.6667,  1.7009],\n","         [ 1.5810,  1.4269,  1.4783,  ...,  1.7009,  1.6153,  1.6495],\n","         [ 1.5639,  1.4098,  1.4440,  ...,  1.6838,  1.5982,  1.6324],\n","         ...,\n","         [-0.4739, -0.4911, -0.5424,  ...,  1.2557,  1.1872,  1.2214],\n","         [-0.5596, -0.4911, -0.4911,  ...,  1.2385,  1.1872,  1.2214],\n","         [-0.6281, -0.3883, -0.3369,  ...,  1.2385,  1.1872,  1.2214]],\n","\n","        [[ 1.8333,  1.6758,  1.7283,  ...,  1.9209,  1.8333,  1.8683],\n","         [ 1.7458,  1.5882,  1.6408,  ...,  1.8683,  1.7808,  1.8158],\n","         [ 1.7283,  1.5707,  1.6057,  ...,  1.8508,  1.7633,  1.7983],\n","         ...,\n","         [-0.5826, -0.6001, -0.6527,  ...,  1.4132,  1.3431,  1.3431],\n","         [-0.6702, -0.6001, -0.6001,  ...,  1.3957,  1.3431,  1.3431],\n","         [-0.7402, -0.4951, -0.4426,  ...,  1.3957,  1.3431,  1.3431]],\n","\n","        [[ 2.0474,  1.8905,  1.9428,  ...,  2.1346,  2.0474,  2.0823],\n","         [ 1.9603,  1.8034,  1.8557,  ...,  2.0823,  1.9951,  2.0300],\n","         [ 1.9428,  1.7860,  1.8208,  ...,  2.0648,  1.9777,  2.0125],\n","         ...,\n","         [-0.6367, -0.6541, -0.7064,  ...,  1.6291,  1.5594,  1.5768],\n","         [-0.7238, -0.6541, -0.6541,  ...,  1.6117,  1.5594,  1.5768],\n","         [-0.7936, -0.5495, -0.4973,  ...,  1.6117,  1.5594,  1.5768]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8))\n"]}]},{"cell_type":"markdown","source":["## 데이터 로더 작성"],"metadata":{"id":"w6K2zAvZ1Cjb"}},{"cell_type":"code","source":["batch_size = 8\n","\n","train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 사전 오브젝트로 정리\n","dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n","\n","# 동작 확인\n","batch_iterator = iter(dataloaders_dict[\"val\"])  # 반복자로 변환\n","imges, anno_class_imges = next(batch_iterator)  # 첫 번째 요소 꺼내기\n","print(imges.size())                             # torch.Size([8, 3, 475, 475])\n","print(anno_class_imges.size())                  # torch.Size([8, 3, 475, 475])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFrHUW8_1Dzr","executionInfo":{"status":"ok","timestamp":1664162796122,"user_tz":-540,"elapsed":328,"user":{"displayName":"J지원","userId":"16050027595319260781"}},"outputId":"4078ac66-5b92-48dd-8fda-2d81c8986e11"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 3, 475, 475])\n","torch.Size([8, 475, 475])\n"]}]}]}